{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556853a17d10f05",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# PySpark Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PySpark Session"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5113c0f74268e01"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"tutorial\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:41:46.154047Z",
     "start_time": "2024-01-30T09:41:44.437874Z"
    }
   },
   "id": "e963253f70c514a7",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+--------+------+\n",
      "|               Name|Age|Nickname|Gender|\n",
      "+-------------------+---+--------+------+\n",
      "|       Agus Richard| 26| Richard|  Male|\n",
      "|Damara Astiningtyas| 23|     Ara|Female|\n",
      "+-------------------+---+--------+------+\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Agus Richard\", 26, \"Richard\", \"Male\"),\n",
    "    (\"Damara Astiningtyas\", 23, \"Ara\", \"Female\")\n",
    "]\n",
    "column = [\"Name\", \"Age\", \"Nickname\", \"Gender\"]\n",
    "\n",
    "df = spark.createDataFrame(data, column)\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:41:56.460575Z",
     "start_time": "2024-01-30T09:41:48.085026Z"
    }
   },
   "id": "536a51f4e3f4db16",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               Name|\n",
      "+-------------------+\n",
      "|       Agus Richard|\n",
      "|Damara Astiningtyas|\n",
      "+-------------------+\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"sample_table\")\n",
    "df2 = spark.sql(\"SELECT Name FROM sample_table\")\n",
    "df2.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:41:58.219367Z",
     "start_time": "2024-01-30T09:41:56.448607Z"
    }
   },
   "id": "79bfe439f8f89193",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "SparkRuntimeException",
     "evalue": "[LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`default`.`sample_hive_table`, as its associated location 'file:/Users/agusrichard/Documents/personal/workbook/data-engineering-workbook/pyspark-tutorial/playground/spark-warehouse/sample_hive_table' already exists. Please pick a different table name, or remove the existing location first.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mSparkRuntimeException\u001B[0m                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msample_table\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msample_hive_table\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/personal/workbook/data-engineering-workbook/pyspark-tutorial/playground/venv/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1586\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1584\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1585\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[0;32m-> 1586\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/personal/workbook/data-engineering-workbook/pyspark-tutorial/playground/venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/Documents/personal/workbook/data-engineering-workbook/pyspark-tutorial/playground/venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mSparkRuntimeException\u001B[0m: [LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`default`.`sample_hive_table`, as its associated location 'file:/Users/agusrichard/Documents/personal/workbook/data-engineering-workbook/pyspark-tutorial/playground/spark-warehouse/sample_hive_table' already exists. Please pick a different table name, or remove the existing location first."
     ]
    }
   ],
   "source": [
    "spark.table(\"sample_table\").write.saveAsTable(\"sample_hive_table\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:41:59.301501Z",
     "start_time": "2024-01-30T09:41:58.194495Z"
    }
   },
   "id": "d48e50bbda86796d",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df3 = spark.sql(\"SELECT * FROM sample_hive_table\")\n",
    "df3.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-30T09:41:59.285270Z"
    }
   },
   "id": "f0f68b0d6a1b68d0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dbs = spark.catalog.listDatabases()\n",
    "print(dbs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-30T09:41:59.287313Z"
    }
   },
   "id": "aef0f535d81ca63e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tables = spark.catalog.listTables()\n",
    "for table in tables:\n",
    "    print(table.name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-30T09:41:59.289296Z"
    }
   },
   "id": "68e875c395d6695c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PySpark RDD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d78a1db9b68dfd12"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pyspark-rdd\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-30T09:41:59.291102Z"
    }
   },
   "id": "e139649bebde3c5e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = list(range(100))\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-30T09:41:59.293104Z"
    }
   },
   "id": "2243cfe1f83ad12b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(data, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-30T09:41:59.294857Z"
    }
   },
   "id": "5c3e67fc7298f4f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rdd.getNumPartitions()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-30T09:41:59.296033Z"
    }
   },
   "id": "8771d9ae9f4888f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(\"./test.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-30T09:41:59.297065Z"
    }
   },
   "id": "ddeee52bf2d7cfcf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rdd2 = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "rdd3 = rdd2.map(lambda x: (x, 1))\n",
    "rdd4 = rdd3.map(lambda x: (x[1], x[0])).sortByKey()\n",
    "print(rdd4.collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-30T09:41:59.298165Z"
    }
   },
   "id": "b467d827e2d431a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rdd4.saveAsTextFile(\"result_test\")\n",
    "# somethihgn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-30T09:41:59.299343Z"
    }
   },
   "id": "c4d33268fcdb37bd",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Repartition and Coalesce"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4578ee8d74bae453"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/30 16:44:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"repartition-coalesce\").master(\"local[4]\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:44:03.252819Z",
     "start_time": "2024-01-30T09:44:03.209691Z"
    }
   },
   "id": "16e981471f1d75e1",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N partitions:  8\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(list(range(1000)))\n",
    "print(\"N partitions: \", rdd.getNumPartitions())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:44:04.349870Z",
     "start_time": "2024-01-30T09:44:04.316776Z"
    }
   },
   "id": "76d923b0cff4dc4c",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N partitions:  4\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.repartition(4)\n",
    "print(\"N partitions: \", rdd2.getNumPartitions())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:44:27.638334Z",
     "start_time": "2024-01-30T09:44:27.459981Z"
    }
   },
   "id": "b5f53fac483c0f3a",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd2.saveAsTextFile(\"./tmp/repartition\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:45:37.893314Z",
     "start_time": "2024-01-30T09:45:36.899678Z"
    }
   },
   "id": "c94461aa48ce784d",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Broadcast and Accumulator Variables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "252b1248e2c7d3cf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/30 17:04:09 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T10:04:10.030520Z",
     "start_time": "2024-01-30T10:04:09.487172Z"
    }
   },
   "id": "3a19bfd00dd3e03",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/30 17:04:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"accumulator\").getOrCreate()\n",
    "\n",
    "accum=spark.sparkContext.accumulator(0)\n",
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)\n",
    "\n",
    "accuSum=spark.sparkContext.accumulator(0)\n",
    "def countFun(x):\n",
    "    global accuSum\n",
    "    accuSum+=x\n",
    "rdd.foreach(countFun)\n",
    "print(accuSum.value)\n",
    "\n",
    "accumCount=spark.sparkContext.accumulator(0)\n",
    "rdd2=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd2.foreach(lambda x:accumCount.add(1))\n",
    "print(accumCount.value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T10:04:27.668467Z",
     "start_time": "2024-01-30T10:04:25.817247Z"
    }
   },
   "id": "1b28227ffa937e9e",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "325bdc295e246fed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
